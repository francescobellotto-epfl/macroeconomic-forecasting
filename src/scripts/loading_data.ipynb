{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import datetime \n",
    "import importlib\n",
    "import utils\n",
    "import torch\n",
    "import ast\n",
    "# import wrds\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, \\\n",
    "BertTokenizer, BertModel, LongformerTokenizer, LongformerModel\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of useful macroeconomic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we load macroeconomic information from WRDS through some SQL queries. This way, we will end up with the file 'crsp_monthly.csv' (in data folder), containing in particular all companies'market capitalizations at the time of their earnings calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2009-01-01'\n",
    "end_date= '2020-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to wrds: to run this, a WRDS account is needed\n",
    "conn = wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get S&P500 Index Membership from CRSP\n",
    "###  monthly frequency of the data, \n",
    "\n",
    "sp500 = conn.raw_sql(\"\"\"\n",
    "                        select a.*, b.date, b.ret\n",
    "                        from crsp.msp500list as a,\n",
    "                        crsp.msf as b\n",
    "                        where a.permno=b.permno\n",
    "                        and b.date >= a.start and b.date<= a.ending\n",
    "                        and b.date>='02/15/2005' \n",
    "                        order by date;\n",
    "                        \"\"\", date_cols=['start', 'ending', 'date']) \n",
    "\n",
    "# Save data\n",
    "sp500.to_csv('sp500.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add Other Company Identifiers from CRSP.MSENAMES\n",
    "### - You don't need this step if only PERMNO is required\n",
    "### - This step aims to add TICKER, SHRCD, EXCHCD and etc. \n",
    "# chiedere\n",
    "\n",
    "mse = conn.raw_sql(\"\"\"\n",
    "                        select comnam, ncusip, namedt, nameendt, \n",
    "                        permno, shrcd, exchcd, hsiccd, ticker\n",
    "                        from crsp.msenames\n",
    "                   \n",
    "                        \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "\n",
    "# if nameendt is missing then set to today date\n",
    "mse['nameendt']=mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "\n",
    "# Save data\n",
    "mse.to_csv('mse.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with SP500 data\n",
    "sp500_full = pd.merge(sp500, mse, how = 'left', on = 'permno')\n",
    "\n",
    "# Impose the date range restrictions\n",
    "sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                            & (sp500_full.date<=sp500_full.nameendt)]\n",
    "\n",
    "# Save data\n",
    "sp500_full.to_csv('sp500_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add Compustat Identifiers\n",
    "### - Link with Compustat's GVKEY and IID if need to work with \n",
    "###   fundamental data\n",
    "### - Linkage is done through crsp.ccmxpf_linktable\n",
    "\n",
    "ccm=conn.raw_sql(\"\"\"\n",
    "                  select gvkey, liid as iid, lpermno as permno,\n",
    "                  linktype, linkprim, linkdt, linkenddt\n",
    "                  from crsp.ccmxpf_linktable\n",
    "                  where substr(linktype,1,1)='L'\n",
    "                  and (linkprim ='C' or linkprim='P')\n",
    "                  \"\"\", date_cols=['linkdt', 'linkenddt'])\n",
    "\n",
    "# if linkenddt is missing then set to today date\n",
    "ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "# Save data\n",
    "ccm.to_csv('ccm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the CCM data with S&P500 data\n",
    "# First just link by matching PERMNO\n",
    "sp500ccm = pd.merge(sp500_full, ccm, how='left', on=['permno'])\n",
    "\n",
    "# Then set link date bounds\n",
    "sp500ccm = sp500ccm.loc[(sp500ccm['date']>=sp500ccm['linkdt'])\\\n",
    "                        &(sp500ccm['date']<=sp500ccm['linkenddt'])]\n",
    "\n",
    "# Rearrange columns for final output\n",
    "sp500ccm = sp500ccm.drop(columns=['namedt', 'nameendt', 'linktype', \\\n",
    "                                  'linkprim', 'linkdt', 'linkenddt'])\n",
    "sp500ccm = sp500ccm[['date', 'permno', 'comnam', 'ncusip',\\\n",
    "                     'shrcd', 'exchcd', 'hsiccd', 'ticker', \\\n",
    "                     'gvkey', 'iid', 'start', 'ending', 'ret']]\n",
    "\n",
    "# Save data\n",
    "sp500ccm.to_csv('sp500ccm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add CIKs and Link with SEC Index Files using CIK\n",
    "names = conn.raw_sql(\"\"\" select gvkey, cik, sic from comp.names \"\"\")\n",
    "\n",
    "# Merge sp500 constituents table with names table\n",
    "sp500cik = pd.merge(sp500ccm, names, on='gvkey',  how='left')\n",
    "\n",
    "# Save data\n",
    "sp500cik.to_csv('sp500cik.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting permno and gvkey to use as identifiers for the next steps\n",
    "per_gev= sp500cik[['permno', 'gvkey']].drop_duplicates()\n",
    "# select the relvant permno to extract only the information about the companies in the sp500\n",
    "selected_permno = sp500cik['permno'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Query\n",
    "crsp_monthly= conn.raw_sql(f\"\"\"\n",
    "    SELECT msf.permno,  msf.mthcaldt AS date,\n",
    "           msf.mthret AS ret, msf.shrout, msf.mthprc AS altprc,\n",
    "           ssih.primaryexch, ssih.siccd\n",
    "    FROM crsp.msf_v2 AS msf\n",
    "    INNER JOIN crsp.stksecurityinfohist AS ssih\n",
    "    ON msf.permno = ssih.permno\n",
    "    AND ssih.secinfostartdt <= msf.mthcaldt\n",
    "    AND msf.mthcaldt <= ssih.secinfoenddt\n",
    "    WHERE msf.mthcaldt BETWEEN '{start_date}' AND '{end_date}'\n",
    "    and msf.permno in {tuple(selected_permno)}\n",
    "    AND ssih.sharetype = 'NS'\n",
    "    AND ssih.securitytype = 'EQTY'\n",
    "    AND ssih.securitysubtype = 'COM'\n",
    "    AND ssih.usincflg = 'Y'\n",
    "    AND ssih.issuertype IN ('ACOR', 'CORP')\n",
    "    AND ssih.primaryexch IN ('N', 'A', 'Q')\n",
    "    AND ssih.conditionaltype IN ('RW', 'NW')\n",
    "    AND ssih.tradingstatusflg = 'A';\n",
    "\"\"\")\n",
    "\n",
    "# create a new column for market capitalization\n",
    "crsp_monthly = crsp_monthly.assign(mktcap=crsp_monthly[\"shrout\"] * crsp_monthly[\"altprc\"] * 1000)\n",
    "crsp_monthly['date'] = pd.to_datetime(crsp_monthly['date'])\n",
    "crsp_monthly=crsp_monthly.sort_values(by=['date'], ascending=True)\n",
    "crsp_monthly['date'] = pd.to_datetime(crsp_monthly['date'])\n",
    "\n",
    "# Create lagged marketcap\n",
    "mktcap_lag = (\n",
    "   crsp_monthly\n",
    "    .assign(\n",
    "        date=lambda x: x[\"date\"] + pd.DateOffset(months=1),\n",
    "        mktcap_lag=lambda x: x[\"mktcap\"]\n",
    "    )\n",
    "    .get([\"permno\", \"date\", \"mktcap_lag\"])\n",
    ")\n",
    "crsp_monthly= pd.merge(crsp_monthly, mktcap_lag, how=\"left\", on=[\"permno\", \"date\"]) \n",
    "crsp_monthly.dropna(subset=['mktcap_lag'], inplace=True)\n",
    "crsp_monthly=crsp_monthly.reset_index(drop=True)\n",
    "crsp_monthly = crsp_monthly.merge(per_gev, on='permno', how='left')\n",
    "\n",
    "# Save data\n",
    "crsp_monthly.to_csv('crsp_monthly_correct.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed with the creation of the final datasets that we will use to run the models. Specifically, the required tasks are:\n",
    "- Tokenize all available texts using BERT, DistilBERT and Longformer tokenizers;\n",
    "- From the tokenized texts, create their embeddings through the corresponding models (resp. BERT, DistilBERT and Longformer);\n",
    "- Extract only CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load useful dataset\n",
    "dataset = pd.read_parquet('data_4ml.parquet')\n",
    "crsp = pd.read_csv('crsp_monthly_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gvkey to dataset\n",
    "gvkey_500 = [str(num) for num in crsp['gvkey']]\n",
    "dataset['gvkey'] = dataset['gvkey'].apply(lambda x: x.lstrip(\"0\"))\n",
    "dataset = dataset[dataset['gvkey'].isin(gvkey_500)]\n",
    "\n",
    "# Group by 'transcriptid' and concatenate 'componenttext' values, handling None values\n",
    "# create one unique row for each transcript id\n",
    "dataset = (\n",
    "    dataset.groupby('transcriptid', as_index=False)\n",
    "    .agg({'componenttext': lambda x: ' '.join(filter(None, x)), 'mostimportantdateutc': 'first', 'gvkey': 'first'}) \n",
    ")\n",
    "\n",
    "# Deal with time\n",
    "dataset['mostimportantdateutc'] = pd.to_datetime(dataset['mostimportantdateutc'])\n",
    "dataset['month'] = dataset['mostimportantdateutc'].dt.month\n",
    "dataset['year'] = dataset['mostimportantdateutc'].dt.year\n",
    "dataset.drop(columns=['mostimportantdateutc'], inplace=True)\n",
    "\n",
    "# Save results\n",
    "dataset.to_csv('data/dataset_with_texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DistilBERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Phase ...\n",
    "\n",
    "texts = pd.read_csv(\"data/dataset_with_texts.csv\")\n",
    "texts_to_tokenize = [row ['componenttext'] for _, row in texts.iterrows()]\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenized_texts = tokenizer(texts_to_tokenize, max_length=512, \n",
    "                            padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "ids_list = []\n",
    "for i in range(len(tokenized_texts['input_ids'])):\n",
    "    ids_list.append(tokenized_texts['input_ids'][i].tolist())\n",
    "\n",
    "mask_list = []\n",
    "for i in range(len(tokenized_texts['attention_mask'])):\n",
    "    mask_list.append(tokenized_texts['attention_mask'][i].tolist())\n",
    "\n",
    "texts['tokenized_text'] = ids_list\n",
    "texts['attention_mask'] = mask_list\n",
    "\n",
    "texts.to_csv('data/embeddings_distilbert_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings ...\n",
    "\n",
    "data = pd.read_csv('data/embeddings_distilbert_correct.csv')\n",
    "\n",
    "distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to('cuda')\n",
    "\n",
    "def get_embedding(row):\n",
    "    tok_text_list = [ast.literal_eval(row['tokenized_text'])]\n",
    "    att_list = [ast.literal_eval(row['attention_mask'])]\n",
    "    texts_tok = torch.tensor(tok_text_list, device='cuda')\n",
    "    texts_att = torch.tensor(att_list, device='cuda')\n",
    "    with torch.no_grad():\n",
    "        embedding = distilbert(input_ids=texts_tok, attention_mask=texts_att)\n",
    "    hidden_state = embedding.last_hidden_state[:, 0, :] \n",
    "    hidden_state = hidden_state.to('cpu')\n",
    "    return hidden_state.detach().numpy()\n",
    "\n",
    "chunk_size = 100\n",
    "\n",
    "for start in range(0, len(dataset), chunk_size):\n",
    "    try:\n",
    "        end = start + chunk_size\n",
    "        chunk = dataset.iloc[start:end] \n",
    "        dataset.loc[start:end-1, 'embeddings'] = chunk.apply(get_embedding, axis=1)\n",
    "        # ds[['embeddings']].to_csv('last_results_bert_correct.csv')\n",
    "        print(f'Chunk {start}-{end} done')\n",
    "    except:\n",
    "        print(f\"Could not retrieve embedding for chunk {start}-{end}\")\n",
    "\n",
    "dataset.to_csv('data/embeddings_distilbert_correct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) BERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Phase ...\n",
    "\n",
    "texts = pd.read_csv(\"data/dataset_with_texts.csv\")\n",
    "texts_to_tokenize = [row ['componenttext'] for _, row in texts.iterrows()]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = tokenizer(texts_to_tokenize, max_length=512, \n",
    "                            padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "ids_list = []\n",
    "for i in range(len(tokenized_texts['input_ids'])):\n",
    "    ids_list.append(tokenized_texts['input_ids'][i].tolist())\n",
    "\n",
    "mask_list = []\n",
    "for i in range(len(tokenized_texts['attention_mask'])):\n",
    "    mask_list.append(tokenized_texts['attention_mask'][i].tolist())\n",
    "\n",
    "texts['tokenized_text'] = ids_list\n",
    "texts['attention_mask'] = mask_list\n",
    "\n",
    "texts.to_csv('data/embeddings_bert_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings ...\n",
    "\n",
    "data = pd.read_csv('data/embeddings_bert_correct.csv')\n",
    "\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to('cuda')\n",
    "\n",
    "def get_embedding(row):\n",
    "    tok_text_list = [ast.literal_eval(row['tokenized_text'])]\n",
    "    att_list = [ast.literal_eval(row['attention_mask'])]\n",
    "    texts_tok = torch.tensor(tok_text_list, device='cuda')\n",
    "    texts_att = torch.tensor(att_list, device='cuda')\n",
    "    with torch.no_grad():\n",
    "        embedding = bert(input_ids=texts_tok, attention_mask=texts_att)\n",
    "    hidden_state = embedding.last_hidden_state[:, 0, :] \n",
    "    hidden_state = hidden_state.to('cpu')\n",
    "    return hidden_state.detach().numpy()\n",
    "\n",
    "chunk_size = 100\n",
    "\n",
    "for start in range(0, len(dataset), chunk_size):\n",
    "    try:\n",
    "        end = start + chunk_size\n",
    "        chunk = dataset.iloc[start:end] \n",
    "        dataset.loc[start:end-1, 'embeddings'] = chunk.apply(get_embedding, axis=1)\n",
    "        # ds[['embeddings']].to_csv('last_results_bert_correct.csv')\n",
    "        print(f'Chunk {start}-{end} done')\n",
    "    except:\n",
    "        print(f\"Could not retrieve embedding for chunk {start}-{end}\")\n",
    "\n",
    "dataset.to_csv('data/embeddings_bert_correct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Longformer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Phase ...\n",
    "\n",
    "texts = pd.read_csv(\"data/dataset_with_texts.csv\")\n",
    "texts_to_tokenize = [row ['componenttext'] for _, row in texts.iterrows()]\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "tokenized_texts = tokenizer(texts_to_tokenize, max_length=4096, \n",
    "                            padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "ids_list = []\n",
    "for i in range(len(tokenized_texts['input_ids'])):\n",
    "    ids_list.append(tokenized_texts['input_ids'][i].tolist())\n",
    "\n",
    "mask_list = []\n",
    "for i in range(len(tokenized_texts['attention_mask'])):\n",
    "    mask_list.append(tokenized_texts['attention_mask'][i].tolist())\n",
    "\n",
    "texts['tokenized_text'] = ids_list\n",
    "texts['attention_mask'] = mask_list\n",
    "\n",
    "texts.to_csv('data/embeddings_longformer_correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings ...\n",
    "\n",
    "data = pd.read_csv('data/embeddings_longformer_correct.csv')\n",
    "\n",
    "longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096').to('cuda')\n",
    "\n",
    "def get_embedding(row):\n",
    "    tok_text_list = [ast.literal_eval(row['tokenized_text'])]\n",
    "    att_list = [ast.literal_eval(row['attention_mask'])]\n",
    "    texts_tok = torch.tensor(tok_text_list, device='cuda')\n",
    "    texts_att = torch.tensor(att_list, device='cuda')\n",
    "    with torch.no_grad():\n",
    "        embedding = longformer(input_ids=texts_tok, attention_mask=texts_att)\n",
    "    hidden_state = embedding.last_hidden_state[:, 0, :] \n",
    "    hidden_state = hidden_state.to('cpu')\n",
    "    return hidden_state.detach().numpy()\n",
    "\n",
    "chunk_size = 100\n",
    "\n",
    "for start in range(0, len(dataset), chunk_size):\n",
    "    try:\n",
    "        end = start + chunk_size\n",
    "        chunk = dataset.iloc[start:end] \n",
    "        dataset.loc[start:end-1, 'embeddings'] = chunk.apply(get_embedding, axis=1)\n",
    "        # ds[['embeddings']].to_csv('last_results_bert_correct.csv')\n",
    "        print(f'Chunk {start}-{end} done')\n",
    "    except:\n",
    "        print(f\"Could not retrieve embedding for chunk {start}-{end}\")\n",
    "\n",
    "dataset.to_csv('data/embeddings_longformer_correct.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
